{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Make sure in a python 3.8 env\n",
    "# pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # bringing the holstic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction - detecting using mediapipe\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "# CAN CHANGE THE COLOURS OF THESE TO MAKE IT DIFFERENT \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "# Extracting data points\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.000.mp4 0\n",
      "Directory Already Exists, passing file\n",
      "B.001.mp4 1\n",
      "Directory Already Exists, passing file\n",
      "B.002.mp4 2\n",
      "Directory Already Exists, passing file\n",
      "B.003.mp4 3\n",
      "Directory Already Exists, passing file\n",
      "B.004.mp4 4\n",
      "Directory Already Exists, passing file\n",
      "B.005.mp4 5\n",
      "Directory Already Exists, passing file\n",
      "B.006.mp4 6\n",
      "Directory Already Exists, passing file\n",
      "B.007.mp4 7\n",
      "Directory Already Exists, passing file\n",
      "B.008.mp4 8\n",
      "Directory Already Exists, passing file\n",
      "B.009.mp4 9\n",
      "Directory Already Exists, passing file\n",
      "B.010.mp4 10\n",
      "Directory Already Exists, passing file\n",
      "B.011.mp4 11\n",
      "Directory Already Exists, passing file\n",
      "B.012.mp4 12\n",
      "Directory Already Exists, passing file\n",
      "B.013.mp4 13\n",
      "Directory Already Exists, passing file\n",
      "B.014.mp4 14\n",
      "Directory Already Exists, passing file\n",
      "B.015.mp4 15\n",
      "Directory Already Exists, passing file\n",
      "B.016.mp4 16\n",
      "Directory Already Exists, passing file\n",
      "B.017.mp4 17\n",
      "Directory Already Exists, passing file\n",
      "B.018.mp4 18\n",
      "Directory Already Exists, passing file\n",
      "B.019.mp4 19\n",
      "Directory Already Exists, passing file\n",
      "B.020.mp4 20\n",
      "Directory Already Exists, passing file\n",
      "B.021.mp4 21\n",
      "Directory Already Exists, passing file\n",
      "B.022.mp4 22\n",
      "Directory Already Exists, passing file\n",
      "B.023.mp4 23\n",
      "Directory Already Exists, passing file\n",
      "C.000.mp4 24\n",
      "Directory Already Exists, passing file\n",
      "C.001.mp4 25\n",
      "Directory Already Exists, passing file\n",
      "C.002.mp4 26\n",
      "Directory Already Exists, passing file\n",
      "C.003.mp4 27\n",
      "Directory Already Exists, passing file\n",
      "C.004.mp4 28\n",
      "Directory Already Exists, passing file\n",
      "C.005.mp4 29\n",
      "Directory Already Exists, passing file\n",
      "C.006.mp4 30\n",
      "Directory Already Exists, passing file\n",
      "C.007.mp4 31\n",
      "Directory Already Exists, passing file\n",
      "C.008.mp4 32\n",
      "Directory Already Exists, passing file\n",
      "C.009.mp4 33\n",
      "Directory Already Exists, passing file\n",
      "C.010.mp4 34\n",
      "Directory Already Exists, passing file\n",
      "C.011.mp4 35\n",
      "Directory Already Exists, passing file\n",
      "C.012.mp4 36\n",
      "Directory Already Exists, passing file\n",
      "C.013.mp4 37\n",
      "Directory Already Exists, passing file\n",
      "C.014.mp4 38\n",
      "Directory Already Exists, passing file\n",
      "C.015.mp4 39\n",
      "Directory Already Exists, passing file\n",
      "C.016.mp4 40\n",
      "Directory Already Exists, passing file\n",
      "C.017.mp4 41\n",
      "Directory Already Exists, passing file\n",
      "C.018.mp4 42\n",
      "Directory Already Exists, passing file\n",
      "C.019.mp4 43\n",
      "Directory Already Exists, passing file\n",
      "C.020.mp4 44\n",
      "Directory Already Exists, passing file\n",
      "C.021.mp4 45\n",
      "Directory Already Exists, passing file\n",
      "C.022.mp4 46\n",
      "Directory Already Exists, passing file\n",
      "C.023.mp4 47\n",
      "Directory Already Exists, passing file\n",
      "C.024.mp4 48\n",
      "Directory Already Exists, passing file\n",
      "C.025.mp4 49\n",
      "Directory Already Exists, passing file\n",
      "F.000.mp4 50\n",
      "Directory Already Exists, passing file\n",
      "F.001.mp4 51\n",
      "Directory Already Exists, passing file\n",
      "F.002.mp4 52\n",
      "Directory Already Exists, passing file\n",
      "F.003.mp4 53\n",
      "Directory Already Exists, passing file\n",
      "F.004.mp4 54\n",
      "Directory Already Exists, passing file\n",
      "F.005.mp4 55\n",
      "Directory Already Exists, passing file\n",
      "F.006.mp4 56\n",
      "Directory Already Exists, passing file\n",
      "F.007.mp4 57\n",
      "Directory Already Exists, passing file\n",
      "F.008.mp4 58\n",
      "Directory Already Exists, passing file\n",
      "F.009.mp4 59\n",
      "Directory Already Exists, passing file\n",
      "F.010.mp4 60\n",
      "Directory Already Exists, passing file\n",
      "F.011.mp4 61\n",
      "Directory Already Exists, passing file\n",
      "F.012.mp4 62\n",
      "Directory Already Exists, passing file\n",
      "F.013.mp4 63\n",
      "Directory Already Exists, passing file\n",
      "F.014.mp4 64\n",
      "Directory Already Exists, passing file\n",
      "F.015.mp4 65\n",
      "Directory Already Exists, passing file\n",
      "F.016.mp4 66\n",
      "Directory Already Exists, passing file\n",
      "F.017.mp4 67\n",
      "Directory Already Exists, passing file\n",
      "F.018.mp4 68\n",
      "Directory Already Exists, passing file\n",
      "F.019.mp4 69\n",
      "Directory Already Exists, passing file\n",
      "F.020.mp4 70\n",
      "Directory Already Exists, passing file\n",
      "F.021.mp4 71\n",
      "Directory Already Exists, passing file\n",
      "F.022.mp4 72\n",
      "Directory Already Exists, passing file\n",
      "F.023.mp4 73\n",
      "Directory Already Exists, passing file\n",
      "F.024.mp4 74\n",
      "Directory Already Exists, passing file\n",
      "F.025.mp4 75\n",
      "Directory Already Exists, passing file\n",
      "F.026.mp4 76\n",
      "Directory Already Exists, passing file\n",
      "F.027.mp4 77\n",
      "Directory Already Exists, passing file\n",
      "F.028.mp4 78\n",
      "Directory Already Exists, passing file\n"
     ]
    }
   ],
   "source": [
    "INPUT_VIDEO_PATH = r'C:\\Users\\Tommaso\\Google Drive\\Current Courses\\COMP9444\\CodingTasks\\Dataset'\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "SEQUENCE_LENGTH = 30\n",
    "\n",
    "label_map = {}\n",
    "no_sequences = {}\n",
    "actions = []\n",
    "# VIDEO NAMING CONVENTION = \"ACTION.SEQUENCE.mp4\" \n",
    "# e.g. \"V.001.mp4\"\n",
    "for i,vid in enumerate(os.listdir(INPUT_VIDEO_PATH)):\n",
    "    print(vid,i)\n",
    "    string = vid.split('.')\n",
    "    action = string[0]\n",
    "    sequence = int(string[1])\n",
    "    cap = cv2.VideoCapture(vid)\n",
    "    label_map[action] = i\n",
    "    \n",
    "    if action in no_sequences:\n",
    "        no_sequences[action] += 1\n",
    "    else:\n",
    "       no_sequences[action] = 1\n",
    "      \n",
    "    if action not in actions:\n",
    "        actions.append(action)\n",
    "    try: \n",
    "        os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "    except:\n",
    "        print('Directory Already Exists, passing file')\n",
    "        continue\n",
    "        \n",
    "    vid_loc = os.path.join(INPUT_VIDEO_PATH,vid)\n",
    "    cap = cv2.VideoCapture(vid_loc)\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # this is code for reading from a video file\n",
    "        frame_num = 0\n",
    "        while(cap.isOpened() and frame_num < SEQUENCE_LENGTH):\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                cv2.waitKey(1)\n",
    "                \n",
    "                # UNCOMMENT THIS TO SEE THE VIDEO DISPLAYED\n",
    "                # cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # # # Show to screen\n",
    "                # cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                frame_num += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 24, 'C': 26, 'F': 29}\n",
      "['B', 'C', 'F']\n",
      "{'B': 0, 'C': 1, 'F': 2}\n"
     ]
    }
   ],
   "source": [
    "print(no_sequences)\n",
    "print(actions)\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(20):\n",
    "        window = []\n",
    "        for frame_num in range(SEQUENCE_LENGTH):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence+1), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "print(label_map)\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 30, 1662)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reasons for doing this\n",
    "# - less data to produce a hyper accurate model\n",
    "# - much denser neural network (rather than 30 40 million paramters have BLANK)\n",
    "# - It was a whole heap faster in detecting in real time\n",
    "print(X.shape)\n",
    "actions = np.asarray(actions)\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "#model.summary()\n",
    "#model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Validating and Predicting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)\n",
    "print(actions[np.argmax(res[0])])\n",
    "print(actions[np.argmax(y_test[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5A. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5B. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights('action_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Real-time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "B\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prob_viz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tommaso\\Google Drive\\Current Courses\\COMP9444\\CodingTasks\\Perceptron-Heros\\ASL_LSTM\\Action_detection.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=41'>42</a>\u001b[0m         sentence \u001b[39m=\u001b[39m sentence[\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m:]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=43'>44</a>\u001b[0m     \u001b[39m# Viz probabilities\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=44'>45</a>\u001b[0m     image \u001b[39m=\u001b[39m prob_viz(res, actions, image, colors)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=46'>47</a>\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(image, (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m), (\u001b[39m640\u001b[39m, \u001b[39m40\u001b[39m), (\u001b[39m245\u001b[39m, \u001b[39m117\u001b[39m, \u001b[39m16\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=47'>48</a>\u001b[0m cv2\u001b[39m.\u001b[39mputText(image, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(sentence), (\u001b[39m3\u001b[39m,\u001b[39m30\u001b[39m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tommaso/Google%20Drive/Current%20Courses/COMP9444/CodingTasks/Perceptron-Heros/ASL_LSTM/Action_detection.ipynb#ch0000020?line=48'>49</a>\u001b[0m                cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m1\u001b[39m, (\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m, cv2\u001b[39m.\u001b[39mLINE_AA)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prob_viz' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('COMP9444_Ass')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8cc99db97905e8d964123eb0cc6f1281e4b2d339b74c7a1742a5513e3c5ec26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
